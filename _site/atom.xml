<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>~/Wade</title>
 <link href="http://www.keithwade.com/atom.xml" rel="self"/>
 <link href="http://www.keithwade.com/"/>
 <updated>2015-12-27T19:32:57-08:00</updated>
 <id>http://www.keithwade.com</id>
 <author>
   <name>Keith Wade</name>
   <email>keawade@gmail.com</email>
 </author>

 
 <entry>
   <title>Configuring a Hyper-V Cluster</title>
   <link href="http://www.keithwade.com/2015/07/03/Configuring_a_Hyper-V_Cluster/"/>
   <updated>2015-07-03T22:00:00-07:00</updated>
   <id>http://www.keithwade.com/2015/07/03/Configuring_a_Hyper-V_Cluster</id>
   <content type="html">&lt;p&gt;Hyper-V can be a particularly useful tool for consolidating computing resources.
However, while consolidating machines into VMs can lower your hardware overhead
costs, this introduces a single failure point for all your machines. If the Hyper-V
server goes down, all your VMs will go down as well.&lt;/p&gt;

&lt;p&gt;To overcome this issue, we can use Hyper-V’s Failover and Load-balancing tools
to build a Hyper-V cluster. These tools will provide failover for our machines so
that, in the event of a critical failure on one of our Hyper-V nodes, the VMs
running on that node will come back up on another node within seconds.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;install-os&quot;&gt;Install OS&lt;/h2&gt;

&lt;p&gt;Begin by installing Windows Server 2012 R2 Datacenter on each of your nodes.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you are not planning on creating more than two Server 2012 Standard virtual
machines, you can use Server 2012 Standard edition. However, if you need more
Server 2012 Standard instances or if you need Server 2012 Datacenter instances,
you will need to use Server 2012 Datacenter for your cluster nodes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is because of the way Microsoft handles their server licenses
&lt;a href=&quot;http://www.altaro.com/hyper-v/virtual-machine-licensing-hyper-v/&quot;&gt;(More details)&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We don’t need anything fancy here. Just install the OS normally and apply all
updates.&lt;/p&gt;

&lt;h2 id=&quot;network-overview&quot;&gt;Network Overview&lt;/h2&gt;

&lt;p&gt;At a minimum, we need three separate networks: Data, Heartbeat, and iSCSI. The
Data network will handle client connections and Internet connectivity. The
Heartbeat network is an internal network exclusive to the cluster used for
Live Migrations of virtual machines and cluster communications between nodes.
The iSCSI network connects each of our nodes to our remote storage device.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For the purposes of this guide, the remote storage will be located on a
&lt;a href=&quot;http://www.nimblestorage.com/products-technology/products-specs/&quot;&gt;NimbleStorage&lt;/a&gt; SAN.
Nimble recommends using Multipath I/O instead of network teaming for their iSCSI
connections so that is what this guide will be doing. Check with your remote
storage manufacturer and use their recommended best practices.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;configure-network-connections&quot;&gt;Configure Network Connections&lt;/h2&gt;

&lt;p&gt;For this example, each of our cluster nodes has six Network Interface Cards
(NICs) to provide redundancy for each network connection. We are going to use
the NIC Teaming manager to manage our Load Balancing and Failover (LBFO) for
our duplicate NICs.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The NIC Teaming manager can be opened by running &lt;em&gt;lbfoadmin&lt;/em&gt; from the Start
  menu or by clicking the link next to NIC Teaming in &lt;em&gt;Server Manager &amp;gt; Local
  Server&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Lets team our Data and Heartbeat connections. Our iSCSI connections will use
Multipath I/O instead. &lt;a href=&quot;/2015/07/06/Load_Balancing_and_Failover_on_Server_2012/&quot;&gt;Instructions NIC teaming can be found here.&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The six NICs on my servers are in three pairs of two NICs each. To increase
redundancy, I split my teams across the pairs so if any one pair were to go
down, the other two would be able to handle the traffic until the first pair
could be replaced.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that our teams have been created, we need to configure the addresses of each
connection. The following table shows an example configuration. Note that the
Data, Heartbeat, and iSCSI networks are on separate subnets. Modify this as
necessary for your own network environment.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;NICs&lt;/th&gt;
      &lt;th&gt;Node1&lt;/th&gt;
      &lt;th&gt;Node2&lt;/th&gt;
      &lt;th&gt;Node3&lt;/th&gt;
      &lt;th&gt;Subnet&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Data&lt;/td&gt;
      &lt;td&gt;1, 6&lt;/td&gt;
      &lt;td&gt;192.168.0.11&lt;/td&gt;
      &lt;td&gt;192.168.0.12&lt;/td&gt;
      &lt;td&gt;192.168.0.13&lt;/td&gt;
      &lt;td&gt;/24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Heartbeat&lt;/td&gt;
      &lt;td&gt;2, 3&lt;/td&gt;
      &lt;td&gt;192.168.1.11&lt;/td&gt;
      &lt;td&gt;192.168.1.12&lt;/td&gt;
      &lt;td&gt;192.168.1.13&lt;/td&gt;
      &lt;td&gt;/24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;iSCSI&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;192.168.2.11&lt;/td&gt;
      &lt;td&gt;192.168.2.12&lt;/td&gt;
      &lt;td&gt;192.168.2.13&lt;/td&gt;
      &lt;td&gt;/24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;iSCSI&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;192.168.2.21&lt;/td&gt;
      &lt;td&gt;192.168.2.22&lt;/td&gt;
      &lt;td&gt;192.168.2.23&lt;/td&gt;
      &lt;td&gt;/24&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;miscellaneous&quot;&gt;Miscellaneous&lt;/h2&gt;

&lt;p&gt;Enable Remote Desktop connections to each of your nodes so we can manage them
more easily later. We also need to add each node to our domain. If your remote
storage solution uses Multipath I/O, install that now using this &lt;a href=&quot;https://technet.microsoft.com/en-us/library/hh852172.aspx&quot;&gt;PowerShell command&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Enable-WindowsOptionalFeature –Online –FeatureName MultiPathIO&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you are using a NimbleStorage SAN, you should install their Nimble Connection
utility now. Be sure to select the deselected option when prompted for components
to install.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;I don’t think the nodes &lt;em&gt;must&lt;/em&gt; be members of a domain but it does make several
steps in the install simpler.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;install-hyper-v&quot;&gt;Install Hyper-V&lt;/h2&gt;
&lt;p&gt;To install the Hyper-V service, run the following &lt;a href=&quot;https://technet.microsoft.com/en-us/library/jj205467%28v=wps.630%29.aspx&quot;&gt;PowerShell command&lt;/a&gt; on each
node:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Install-WindowsFeature –Name Hyper-V -IncludeManagementTools -Restart&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This command will install the Hyper-V service with its management tools and
then restart the server.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;configure-virtual-switch&quot;&gt;Configure Virtual Switch&lt;/h2&gt;

&lt;p&gt;We need to create our Hyper-V Virtual Switches. For a smooth cluster installation,
make sure each of these switches is identical on each node. A way to do this
without much hassle is to run the same &lt;a href=&quot;https://technet.microsoft.com/en-us/library/hh848455%28v=wps.630%29.aspx&quot;&gt;PowerShell commands&lt;/a&gt; on each node instead
of manually configuring each switch. Run the following command on each node. If
you are accessing your node via RDP your connection will be interrupted for a few
seconds.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;New-VMSwitch “Data” –NetAdapterName “Data” –AllowManagementOS:$true&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This command will create a new virtual switch named Data that is
connected to the network adapter on your server node named Data. It will create
a new virtual NIC on your node called vData that your local node will use since
the physical NIC is now tied directly to the virtual switch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;install-failover-clustering&quot;&gt;Install Failover Clustering&lt;/h2&gt;

&lt;p&gt;We are nearly ready to cluster our node servers! On each node, run the following
&lt;a href=&quot;https://technet.microsoft.com/en-us/library/jj205467%28v=wps.630%29.aspx&quot;&gt;PowerShell command&lt;/a&gt; to install the Failover Clustering service with its
management tools:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Install-WindowsFeature –Name Failover-Clustering –IncludeManagementTools&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Before we actually cluster our nodes, let’s test our configuration. Run the
following &lt;a href=&quot;https://technet.microsoft.com/en-us/library/ee461026.aspx&quot;&gt;PowerShell command&lt;/a&gt; and check the report for any failures.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Test-Cluster –Node Node1,Node2,Node3 -ReportName &quot;C:\TestClusterReport&quot;&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You are likely to get warnings concerning your cluster’s storage. This is fine.
We will configure the cluster’s shared storage soon.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After reviewing the report and addressing any problems, we are ready to cluster
our nodes! Run the following &lt;a href=&quot;https://technet.microsoft.com/en-us/library/ee460973.aspx&quot;&gt;PowerShell command&lt;/a&gt; to create the cluster:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;New-Cluster –Name Cluster1 –Node Node1,Node2,Node3 –StaticAddress 192.168.0.10&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;configure-cluster-shared-volumes&quot;&gt;Configure Cluster Shared Volumes&lt;/h2&gt;

&lt;p&gt;At this point, you should refer to your remote storage manufacturer’s best
practice documentation for Hyper-V to configure your Cluster Shared Volumes (CSV).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I will describe the steps necessary for a NimbleStorage SAN. If you are using a
different remote storage system, you can skip most of this section and refer to
your manufacturer’s documentation instead.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;I will describe the steps necessary to connect to an existing volume but not how
to create that volume on the SAN. Please refer to &lt;a href=&quot;https://infosight.nimblestorage.com/InfoSight/login&quot;&gt;their documentation&lt;/a&gt; instead.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On each node, open the Nimble Connection Manager and exclude all addresses
except the iSCSI addresses. Make sure each node’s Initiator Name is added to the
SAN’s initiator group. Enter your Nimble’s Discovery IP and switch to the Nimble
Volumes tab. Select the desired volume and click connect.&lt;/p&gt;

&lt;p&gt;On only one of your nodes, open Disk Management and bring the new disk online.
Initialize the disk and format with NTFS. You do not need to select a drive letter. In
the Failover Cluster Manager, right click on Disks and select Add Disk. Select the
available disks you want to add to the cluster’s shared storage and click OK. Finally,
under the disks item in the Failover Cluster Manager, right click on the volume(s)
you just added and select Add to Cluster Shared Volume.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Your new Cluster Shared Volume is now available on each server at
“C:\clustershared\Volume01\”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is recommended to edit the properties of your Cluster Shared
Volumes to change their names from Volume01, Volume02, etc to
match the names of the volumes on your SAN.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;For detailed information about Cluster Shared Volumes, check out
this &lt;a href=&quot;http://blogs.msdn.com/b/clustering/archive/2013/12/02/10473247.aspx&quot;&gt;MSDN article&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;ready-for-roles&quot;&gt;Ready for Roles&lt;/h2&gt;

&lt;p&gt;We are done installing and configuring our Hyper-V cluster!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I will end this article here but I am planning on writing several follow
up articles on managing our new cluster.&lt;/p&gt;
&lt;/blockquote&gt;
</content>
 </entry>
 
 <entry>
   <title>NTP Server Configuration</title>
   <link href="http://www.keithwade.com/2014/08/03/NTP_Configuration/"/>
   <updated>2014-08-03T22:00:00-07:00</updated>
   <id>http://www.keithwade.com/2014/08/03/NTP_Configuration</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Network_Time_Protocol&quot;&gt;Network Time Protocol&lt;/a&gt; (NTP) is used to synchronize computers around the world to allow for better communications on networks with changing latency such as the internet. This is done with a tiered infrastructure of timing devices.&lt;/p&gt;

&lt;p&gt;The top tier, stratum zero, consists of high precision time devices like &lt;a href=&quot;https://en.wikipedia.org/wiki/Atomic_clock&quot;&gt;atomic clocks&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/GPS_clock#GPS_clocks&quot;&gt;global positioning system clocks&lt;/a&gt; that keep extremely precise time. Below this tier is stratum one which consists of servers with their system clocks closely synchronized with an attached stratum zero device. Next are the stratum two servers with synchronization to stratum one servers and so on.&lt;/p&gt;

&lt;p&gt;To avoid overloading stratum one servers with time requests from clients most stratum one servers are not available for public time requests. Instead, many stratum two servers are maintained specifically for the purpose of providing time synchronization to any client machine. These servers are often managed by businesses for their internal networks but many are managed by organizations like universities or the National Institute of Standards and Technology (NIST) for general use.&lt;/p&gt;

&lt;p&gt;The rest of this article will explain how to implement your own time server and configure it.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;planning&quot;&gt;Planning&lt;/h2&gt;

&lt;p&gt;We need to choose public servers to synchronize with. There are several lists we can choose upstream servers from.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.ntp.org/bin/view/Servers/WebHome#Browsing_the_Lists&quot;&gt;NTP.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.pool.ntp.org/&quot;&gt;NTP Pool Project&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tf.nist.gov/tf-cgi/servers.cgi&quot;&gt;NIST Internet Time Servers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are &lt;a href=&quot;http://support.ntp.org/bin/view/Support/SelectingOffsiteNTPServers#Section_5.3.1.&quot;&gt;a few things&lt;/a&gt; to keep in mind when selecting your upstream servers.
1. Do not use any time servers which you do not have permission to use! The severs you select should be:
   * Public time servers made available by your service provider or OS vendor to their customers
   * And/or, other private time servers for which you have been given explicit permission and instructions on proper use
   * And/or, the time servers designated by the pool.ntp.org project
   * And/or, on one of the public lists of time servers
2. Make sure your selected servers are diverse and not maintained by or synchronized with the same organizations.
3. Make sure your selected servers are moderately close to your network location. This is not super critical, but try to avoid servers half way around the world.&lt;/p&gt;

&lt;p&gt;There may be other things specific to your implementation, but this will give you a good start. For more information, check out the &lt;a href=&quot;http://support.ntp.org/bin/view/Support/SelectingOffsiteNTPServers#Section_5.3.1.&quot;&gt;official NTP guidelines&lt;/a&gt; for server selection.&lt;/p&gt;

&lt;p&gt;It is &lt;a href=&quot;http://support.ntp.org/bin/view/Support/SelectingOffsiteNTPServers#Section_5.3.3.&quot;&gt;recommended&lt;/a&gt; that you choose at least five upstream servers to allow your system to protect against up to two &lt;em&gt;falsetickers&lt;/em&gt;, servers that are not reporting the correct time. Depending on your needs, you may want to consider using more.&lt;/p&gt;

&lt;p&gt;For this guide, I’ve selected seven stratum one servers from around the United States to create a stratum two server for my network. When I selected my servers, I took care to only pick servers that are &lt;strong&gt;OpenAccess&lt;/strong&gt; and did not require &lt;strong&gt;NotificationMessage&lt;/strong&gt;. Be careful when selecting your own servers as many require a NotificationMessage but neglected to change their policy to RestrictedAccess as &lt;a href=&quot;http://support.ntp.org/bin/view/Servers/NotificationMessage&quot;&gt;specifically requested&lt;/a&gt; by ntp.org (Grumble, grumble).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.ntp.org/bin/view/Servers/PublicTimeServer000498&quot;&gt;ntp.your.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.ntp.org/bin/view/Servers/PublicTimeServer000280&quot;&gt;time-b.timefreq.bldrdoc.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.ntp.org/bin/view/Servers/PublicTimeServer000290&quot;&gt;rackety.udel.edu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.ntp.org/bin/view/Servers/PublicTimeServer000011&quot;&gt;clock.fmt.he.net&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.ntp.org/bin/view/Servers/PublicTimeServer000787&quot;&gt;gpstime.la-archdiocese.net&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.ntp.org/bin/view/Servers/PublicTimeServer000378&quot;&gt;nist1.columbiacountyga.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.ntp.org/bin/view/Servers/PublicTimeServer000974&quot;&gt;gclock02.dupa01.burst.net&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;

&lt;p&gt;This guide will work with NTP on CentOS. Other Linux distributions will have slight differences but should not be to difficult to configure using this guide.
First, install the Network Time Protocol.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;yum install ntp -y&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You will likely want NTP to start on boot. CentOS 7 uses SystemD to manage services. Use the following command to enable starting on boot.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;systemctl enable ntpd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you are running an older version of CentOS you will need to use chkconfig to do this.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;chkconfig ntpd on&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now our NTP service will run every time the server boots. At this point we’re ready to start configuring the server. We won’t start the service yet, we’ll wait until we have a basic configuration to test first.&lt;/p&gt;

&lt;h2 id=&quot;server-configuration&quot;&gt;Server Configuration&lt;/h2&gt;

&lt;p&gt;Now that you have selected your upstream servers and have installed NTP on your own server, you are ready to start configuring NTP to synchronize. We will first make a backup of your original NTP configuration file so we can look through it later. For now though we will delete the original (But not the backup!), and then create our own config file to work with.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cp /etc/ntp.conf /etc/ntp.conf.bak&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rm /etc/ntp.conf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nano /etc/ntp.conf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you want to return to the original file, just replace the actual file with your backup to start over by running the following command.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cp /etc/ntp.conf.bak /etc/ntp.conf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here is an example ntp.conf file that I’ll be using to explain the basic concepts of the configuration.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Drift file.
driftfile /var/lib/ntp/drift
# Permit
restrict default limited kod nomodify notrap nopeer
restrict -6 default limited kod nomodify notrap nopeer
# Servers
server ntp.your.org iburst
server time-b.timefreq.bldrdoc.gov iburst
server rackety.udel.edu iburst
server clock.fmt.he.net iburst
server gpstime.la-archdiocese.net iburst
server nist1.columbiacountyga.gov iburst
server gclock02.dupa01.burst.net iburst
disable monitor
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets break down each of these pieces and figure out what they do.
First, the &lt;strong&gt;drift file&lt;/strong&gt;. The drift file stores information about your system clock’s drift tendency. Most system clocks keep time reasonably well but still require synchronization in order to remain accurate. The drift file keeps track of how much your system clock tends to drift from actual time. Then, if you lose your connection to your synchronization servers, it can correct itself based on the drift data it has collected. This give you a bit of slack to work with if your connections ever go down.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;restrict&lt;/strong&gt; lines restricts what external machines can do to our server. The &lt;strong&gt;server **lines define the addresses we will attempt to synchronize with. First we declare the server name and then add the **iburst&lt;/strong&gt; parameter. The &lt;a href=&quot;http://doc.ntp.org/4.1.1/confopt.htm&quot;&gt;iburst parameter&lt;/a&gt; has the service send a burst of eight packets instead of only one when the server is unreachable to help speed up the time to acquire a connection.
The &lt;strong&gt;disable monitor&lt;/strong&gt; line &lt;a href=&quot;http://support.ntp.org/bin/view/Main/SecurityNotice#DRDoS_Amplification_Attack_using&quot;&gt;fixes a vulnerability&lt;/a&gt; from January 2014 on older versions of NTP.&lt;/p&gt;

&lt;p&gt;We can now save our config file and start the service!&lt;/p&gt;

&lt;p&gt;&lt;code&gt;systemctl start ntpd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;&lt;code&gt;service ntpd start&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now that the NTP service is running, we need to make sure our firewall is configured to allow incoming NTP traffic so our clients can synchronize with our server.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;firewall-cmd --permanent --add-service=ntp&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;systemctl restart firewalld&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;&lt;code&gt;iptables -A INPUT -p tcp --dport 123 -j ACCEPT&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/sbin/service iptables save&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;service iptables restart&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Once the server is running we can use the command &lt;code&gt;ntpq -p&lt;/code&gt; to list our servers and view details about our connections to them. We can also use &lt;code&gt;ntpstat&lt;/code&gt; to see our current synchronization status. For details on what this data means, check the &lt;a href=&quot;http://doc.ntp.org/4.2.4/ntpq.html&quot;&gt;NTP documentation page for the ntpq command&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;client-configuration&quot;&gt;Client Configuration&lt;/h2&gt;

&lt;p&gt;The client configuration is much simpler than the server configuration was. First, repeat the installation steps for the NTP service. Next, repeat the configuration steps but when you create your new ntp.conf file we will do some things differently.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Drift file.
driftfile /var/lib/ntp/drift
# Permit
restrict default limited kod nomodify notrap nopeer
restrict -6 default limited kod nomodify notrap nopeer
# Servers
server time.mydomain.com iburst
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since our source is known, and likely internal, we can get away with a bit less security. Also, since we don’t need anything to connect to the clients on an incoming port, we don’t need to modify our firewall rules as long as outgoing traffic is unrestricted (As it is by default).
Finally, use the &lt;code&gt;ntpq -p&lt;/code&gt; and &lt;code&gt;ntpstat&lt;/code&gt; commands to verify your NTP service is synchronizing with your server.&lt;/p&gt;

&lt;p&gt;Congratulations! You have just finished configuring your basic NTP server and clients! There are several more things we could do to improve security and redundancy but this is where this particular guide will end.&lt;/p&gt;
</content>
 </entry>
 

</feed>
